package provider

import (
	"context"
	"flag"
	"fmt"
	"log"
	"path/filepath"
	"sort"
	"time"

	"github.com/bazelbuild/bazel-gazelle/config"
	"github.com/bazelbuild/bazel-gazelle/label"
	"github.com/bazelbuild/bazel-gazelle/rule"
	"github.com/bazelbuild/buildtools/build"
	"github.com/rs/zerolog"

	"github.com/stackb/scala-gazelle/pkg/collections"
	"github.com/stackb/scala-gazelle/pkg/parser"
	"github.com/stackb/scala-gazelle/pkg/procutil"
	"github.com/stackb/scala-gazelle/pkg/protobuf"
	"github.com/stackb/scala-gazelle/pkg/resolver"

	sppb "github.com/stackb/scala-gazelle/build/stack/gazelle/scala/parse"
)

const SCALA_GAZELLE_ALLOW_RUNTIME_PARSING = procutil.EnvVar("SCALA_GAZELLE_ALLOW_RUNTIME_PARSING")

const scalaFilesetFileFlagName = "scala_fileset_file"

type progressFunc func(msg string)

// NewSourceProvider constructs a new NewSourceProvider.
func NewSourceProvider(logger zerolog.Logger, progress progressFunc) *SourceProvider {
	return &SourceProvider{
		logger:     logger,
		progress:   progress,
		scalaFiles: make(map[string]*sppb.File),
	}
}

// SourceProvider is provider for scala source files. If -scala_source_index_in
// is configured, the given source index will be used to bootstrap the internal
// cache.  At runtime the .ParseScalaRule function can be used to parse scala
// files.  If the cache already has an entry for the filename with matching
// sha256, the cache hit will be used.
type SourceProvider struct {
	// logger instance
	logger zerolog.Logger
	// progress function
	progress progressFunc
	// scope is the target we provide symbols to
	scope resolver.Scope
	// parser is an instance of the scala source parser.  It is initialized
	// lazily.
	parser *parser.ScalametaParser
	// scalaFilesetFilename is an optional path to a parse.Fileset that provides
	// pre-parsed scala files.
	scalaFilesetFilename string
	// scalaFiles is a mapping that is read from the scalaFilesetFilename, if present
	scalaFiles map[string]*sppb.File
	// hasLifecycleEnded flags whether we have seen the OnEnd() call at least
	// once.  This affects whether we choose to double-check sha256 values
	// during the haveFiles check.  For the first go, assume all files we
	// already have are up-to-date (don't check sha256's again).  If it happens
	// late, assume we are in watch/repair mode and the gazelle process is
	// long-lived, possibly needing to reparse files that could be changed by a
	// developer since the gazelle process strated.
	hasLifecycleEnded bool
}

// Name implements part of the resolver.SymbolProvider interface.
func (r *SourceProvider) Name() string {
	return "source"
}

// RegisterFlags implements part of the resolver.SymbolProvider interface.
func (r *SourceProvider) RegisterFlags(flags *flag.FlagSet, cmd string, c *config.Config) {
	flags.StringVar(&r.scalaFilesetFilename, scalaFilesetFileFlagName, "", "optional path to an imports file where resolved imports should be written (.json or .pb)")
}

// CheckFlags implements part of the resolver.SymbolProvider interface.
func (r *SourceProvider) CheckFlags(flags *flag.FlagSet, c *config.Config, scope resolver.Scope) error {
	if r.scalaFilesetFilename != "" {
		filename := r.scalaFilesetFilename
		if !filepath.IsAbs(filename) {
			filename = filepath.Join(c.WorkDir, filename)
		}
		if err := r.parseScalaFileset(filename); err != nil {
			return err
		}
	}

	r.scope = scope

	return nil
}

// OnResolve implements part of the resolver.SymbolProvider interface.
func (r *SourceProvider) OnResolve() error {
	if r.parser != nil {
		r.parser.Stop()
	}
	return nil
}

// OnEnd implements part of the resolver.SymbolProvider interface.
func (r *SourceProvider) OnEnd() error {
	r.hasLifecycleEnded = true
	return nil
}

// CanProvide implements the resolver.SymbolProvider interface.
func (cr *SourceProvider) CanProvide(dep *resolver.ImportLabel, expr build.Expr, ruleIndex func(from label.Label) (*rule.Rule, bool), from label.Label) bool {
	// if the label points to a rule that was generated by this extension
	if _, ok := ruleIndex(dep.Label); ok {
		return true
	}
	if dep.Label.Relative {
		if _, ok := ruleIndex(dep.Label.Abs(from.Repo, from.Pkg)); ok {
			return true
		}
	}
	return false
}

// ensureParserStarted begins the parser process if it hasn't already.
func (r *SourceProvider) ensureParserStarted() error {
	if r.parser == nil {
		if !procutil.LookupBoolEnv(SCALA_GAZELLE_ALLOW_RUNTIME_PARSING, true) {
			r.logger.Panic().Msg("runtime parsing is disabled")
		}
		r.parser = parser.NewScalametaParser(parser.WithLogger(r.logger.With().Str("parser", "runtime").Logger()))

		now := time.Now()
		r.logger.Printf("[%s] starting parser...", r.Name())

		if err := r.parser.Start(); err != nil {
			return fmt.Errorf("starting parser: %w", err)
		}

		r.logger.Printf("[%s] parser started in %v", r.Name(), time.Since(now))
	}

	return nil
}

// ParseScalaRule implements scalarule.Parser
func (r *SourceProvider) ParseScalaRule(kind string, from label.Label, dir string, srcs ...string) (*sppb.Rule, error) {
	if len(srcs) == 0 {
		return nil, nil
	}
	sort.Strings(srcs)

	files, err := r.parseFiles(dir, srcs, from, kind)
	if err != nil {
		return nil, err
	}
	sort.Slice(files, func(i, j int) bool {
		a := files[i]
		b := files[j]
		return a.Filename < b.Filename
	})

	for _, file := range files {
		if err := r.loadScalaFile(from, kind, file); err != nil {
			return nil, err
		}
	}

	return &sppb.Rule{
		Label: from.String(),
		Kind:  kind,
		Files: files,
	}, nil
}

func (r *SourceProvider) parseFiles(dir string, srcs []string, from label.Label, kind string) ([]*sppb.File, error) {
	// haveFiles is the list of haveFiles we already have from pre-computed
	// scalaFiles whose sha256 values are up-to-date.
	var haveFiles []*sppb.File

	// needFilenames is a mapping from the absolute to relative path of the files we need to parse
	needFilenames := make(map[string]string)
	for _, src := range srcs {
		abs := filepath.Join(dir, src)
		rel := filepath.Join(from.Pkg, src)
		if file, ok := r.scalaFiles[rel]; ok {
			var isUpToDate bool

			if r.hasLifecycleEnded {
				// this is a later pass during gazelle execution such as in
				// watch/repair mode, we need to re-check sha256s
				sha256, err := collections.FileSha256(abs)
				if err != nil {
					return nil, err
				}
				if sha256 == file.Sha256 {
					isUpToDate = true
				}
			} else {
				// this is the (normal) first pass during gazelle execution, no
				// need to re-check sha256s
				isUpToDate = true
			}

			if isUpToDate {
				haveFiles = append(haveFiles, file)
				// log.Println("✅ have:", rel)
				continue
			}
		}
		needFilenames[abs] = rel
	}
	if len(needFilenames) == 0 {
		return haveFiles, nil
	}

	t1 := time.Now()

	if err := r.ensureParserStarted(); err != nil {
		return nil, err
	}

	r.logger.Debug().Msgf("⭕ need to parse: %v", needFilenames)

	filenames := make([]string, 0, len(needFilenames))
	for abs := range needFilenames {
		filenames = append(filenames, abs)
	}
	sort.Strings(filenames)
	response, err := r.parser.Parse(context.Background(), &sppb.ParseRequest{
		Filenames: filenames,
	})
	if err != nil {
		return nil, fmt.Errorf("parse error: %v", err)
	}
	if response.Error != "" {
		return nil, fmt.Errorf("parser error: %s", response.Error)
	}

	t2 := time.Since(t1).Round(1 * time.Millisecond)
	if true {
		log.Printf("Parsed %s%%%s (%d files, %v)", from, kind, len(needFilenames), t2)
	}

	// check for errors and remove dir prefixes.  haveFiles (files thaat come
	// pre-parsed) are workspace-relative.  Ensure that the files we just parsed
	// are also workspace-relative such that they sort similarly.
	for _, file := range response.Files {
		if file.Error != "" {
			return nil, fmt.Errorf("%s parse error: %s", file.Filename, file.Error)
		}
		rel, ok := needFilenames[file.Filename]
		if !ok {
			panic("failed to map parsed file (having absolute path) back to relative path: this is a bug: " + file.Filename)
		}
		file.Filename = rel
		// update saved cache
		r.scalaFiles[rel] = file
	}

	return append(haveFiles, response.Files...), nil
}

// LoadScalaRule loads the given rule state.
func (r *SourceProvider) LoadScalaRule(from label.Label, rule *sppb.Rule) error {
	for _, file := range rule.Files {
		if err := r.loadScalaFile(from, rule.Kind, file); err != nil {
			return err
		}
	}
	return nil
}

func (r *SourceProvider) loadScalaFile(from label.Label, kind string, file *sppb.File) error {
	r.logger.Debug().Msgf("loading symbols from %s: %+v", file.Filename, file)

	for _, imp := range file.Classes {
		r.putSymbol(from, kind, imp, sppb.ImportType_CLASS)
	}
	for _, imp := range file.Objects {
		r.putSymbol(from, kind, imp, sppb.ImportType_OBJECT)
	}
	for _, imp := range file.Traits {
		r.putSymbol(from, kind, imp, sppb.ImportType_TRAIT)
	}
	for _, imp := range file.Types {
		r.putSymbol(from, kind, imp, sppb.ImportType_TYPE)
	}
	for _, imp := range file.Vals {
		r.putSymbol(from, kind, imp, sppb.ImportType_VALUE)
	}
	return nil
}

func (r *SourceProvider) putSymbol(from label.Label, kind, imp string, impType sppb.ImportType) {
	sym := resolver.NewSymbol(impType, imp, kind, from)
	r.logger.Debug().Msgf("adding symbol to scope: %v", sym)
	r.scope.PutSymbol(sym)
}

func (r *SourceProvider) parseScalaFileset(filename string) error {
	r.logger.Debug().Msgf("parsing scala_fileset from %s", filename)
	var ruleset sppb.RuleSet
	if err := protobuf.ReadFile(filename, &ruleset); err != nil {
		return fmt.Errorf("reading --%s: %v", scalaFilesetFileFlagName, err)
	}

	for _, rule := range ruleset.Rules {
		for _, file := range rule.Files {
			r.logger.Trace().Msgf("scala_fileset file: %s", file.Filename)
			r.scalaFiles[file.Filename] = file
		}
	}

	return nil
}
